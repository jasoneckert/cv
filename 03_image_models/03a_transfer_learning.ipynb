{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Order for Ch.3\n",
    "\n",
    "## Core Concepts:\n",
    "1.\t03a_transfer_learning.ipynb → frozen feature extractor (MobileNetV2)\n",
    "2.\t03b_finetune_MOBILENETV2_flowers5.ipynb → lightweight fine-tuning\n",
    "3.\t03b_finetune_experiment_lr_decay_xception_flowers104.ipynb → Xception fine-tuning\n",
    "________________________________________\n",
    "## Extended Concepts:\n",
    "4.\t03c_fromzero_ALEXNET_flowers104.ipynb → small-from-scratch experiment\n",
    "5.\t03g_finetune_RESNET50_flowers104.ipynb → pick one classic large net for fine-tuning\n",
    "6.\t03z_ensemble_finetune_flowers104.ipynb → simplified ensemble (no retraining huge models)\n",
    "\n",
    "***NOTE: These use matplotlib for inline graphs because the confusion matrix for each is too large to visualize in a CSV file with Excel.***\n",
    "________________________________________\n",
    "## Optional / trace-only\n",
    "7.\t03m_transformer_flowers104.ipynb → conceptual, no heavy training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f26lxDv5Srao"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.1\n",
      "Dataset loaded:\n",
      "tfds.core.DatasetInfo(\n",
      "    name='tf_flowers',\n",
      "    full_name='tf_flowers/3.0.1',\n",
      "    description=\"\"\"\n",
      "    A large set of images of flowers\n",
      "    \"\"\",\n",
      "    homepage='https://www.tensorflow.org/tutorials/load_data/images',\n",
      "    data_dir='C:\\\\Users\\\\Jason Eckert\\\\tensorflow_datasets\\\\tf_flowers\\\\3.0.1',\n",
      "    file_format=tfrecord,\n",
      "    download_size=218.21 MiB,\n",
      "    dataset_size=221.83 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(None, None, 3), dtype=uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=5),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    nondeterministic_order=False,\n",
      "    splits={\n",
      "        'train': <SplitInfo num_examples=3670, num_shards=2>,\n",
      "    },\n",
      "    citation=\"\"\"@ONLINE {tfflowers,\n",
      "    author = \"The TensorFlow Team\",\n",
      "    title = \"Flowers\",\n",
      "    month = \"jan\",\n",
      "    year = \"2019\",\n",
      "    url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\" }\"\"\",\n",
      ")\n",
      "Training and validation datasets ready.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense_hidden (Dense)        (None, 16)                20496     \n",
      "                                                                 \n",
      " flower_prob (Dense)         (None, 5)                 85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,278,565\n",
      "Trainable params: 20,581\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "92/92 [==============================] - 29s 282ms/step - loss: 0.8937 - accuracy: 0.6805 - val_loss: 0.4397 - val_accuracy: 0.8665\n",
      "Epoch 2/5\n",
      "92/92 [==============================] - 25s 269ms/step - loss: 0.3734 - accuracy: 0.8828 - val_loss: 0.3258 - val_accuracy: 0.8856\n",
      "Epoch 3/5\n",
      "92/92 [==============================] - 24s 265ms/step - loss: 0.2805 - accuracy: 0.9131 - val_loss: 0.3107 - val_accuracy: 0.8951\n",
      "Epoch 4/5\n",
      "92/92 [==============================] - 26s 283ms/step - loss: 0.2199 - accuracy: 0.9336 - val_loss: 0.2844 - val_accuracy: 0.8951\n",
      "Epoch 5/5\n",
      "92/92 [==============================] - 26s 276ms/step - loss: 0.1839 - accuracy: 0.9458 - val_loss: 0.2503 - val_accuracy: 0.9169\n",
      "Saved training history to 03a_mobilenet_outputs\\mobilenet_training_history.csv\n",
      "Saved predictions to 03a_mobilenet_outputs\\mobilenet_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MobileNet Transfer Learning (TFDS, Local-Friendly Version)\n",
    "# ============================================================\n",
    "#\n",
    "# This notebook demonstrates transfer learning using a\n",
    "# pre-trained MobileNetV2 model and the TFDS `tf_flowers`\n",
    "# dataset. It is designed to run reliably on local machines\n",
    "# (Windows/macOS/Linux) without relying on cloud storage,\n",
    "# GCS paths, or matplotlib.\n",
    "#\n",
    "# Goals:\n",
    "# - Understand transfer learning with CNN feature extractors\n",
    "# - Learn how pre-trained models accelerate training\n",
    "# - Practice working with tf.data pipelines\n",
    "# - Export results for inspection outside Python\n",
    "#\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Imports\n",
    "# ----------------------------\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "\n",
    "CLASS_NAMES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
    "\n",
    "OUTPUT_DIR = \"03a_mobilenet_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load Dataset (TFDS)\n",
    "# ============================================================\n",
    "#\n",
    "# We use TensorFlow Datasets instead of CSV files or cloud\n",
    "# storage paths. This ensures:\n",
    "# - No manual downloads\n",
    "# - No filesystem configuration\n",
    "# - Reproducible experiments\n",
    "#\n",
    "# The tf_flowers dataset contains 5 flower classes.\n",
    "#\n",
    "# ============================================================\n",
    "\n",
    "(ds_train, ds_val), ds_info = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:80%]\", \"train[80%:]\"],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "print(\"Dataset loaded:\")\n",
    "print(ds_info)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Preprocessing Pipeline\n",
    "# ============================================================\n",
    "#\n",
    "# MobileNetV2 expects:\n",
    "# - Images resized to 224x224\n",
    "# - Float32 inputs\n",
    "# - MobileNet-specific preprocessing\n",
    "#\n",
    "# ============================================================\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "train_ds = (\n",
    "    ds_train\n",
    "    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .shuffle(1000)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    ds_val\n",
    "    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(\"Training and validation datasets ready.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Build MobileNet Transfer Learning Model\n",
    "# ============================================================\n",
    "#\n",
    "# We use MobileNetV2 pretrained on ImageNet as a fixed\n",
    "# feature extractor and train a small classification head.\n",
    "#\n",
    "# ============================================================\n",
    "\n",
    "def build_model(num_hidden=16, learning_rate=0.001):\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "    )\n",
    "\n",
    "    # Freeze pretrained weights (transfer learning)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(num_hidden, activation=\"relu\", name=\"dense_hidden\"),\n",
    "        tf.keras.layers.Dense(len(CLASS_NAMES), activation=\"softmax\", name=\"flower_prob\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Train Model\n",
    "# ============================================================\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Save Training History (CSV)\n",
    "# ============================================================\n",
    "#\n",
    "# Instead of plotting with matplotlib, we export metrics\n",
    "# so students can:\n",
    "# - Open them in Excel\n",
    "# - Create their own plots\n",
    "# - Submit them as artifacts\n",
    "#\n",
    "# ============================================================\n",
    "\n",
    "def save_training_history(history, filename):\n",
    "    df = pd.DataFrame(history.history)\n",
    "    path = os.path.join(OUTPUT_DIR, filename)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Saved training history to {path}\")\n",
    "\n",
    "\n",
    "save_training_history(history, \"mobilenet_training_history.csv\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Run Predictions and Save Results\n",
    "# ============================================================\n",
    "#\n",
    "# This replaces image grids with a simple, inspectable CSV:\n",
    "# - True label\n",
    "# - Predicted label\n",
    "# - Model confidence\n",
    "#\n",
    "# ============================================================\n",
    "\n",
    "def predict_and_save(model, dataset, filename, n_samples=20):\n",
    "    results = []\n",
    "\n",
    "    # Work with individual images\n",
    "    unbatched = dataset.unbatch()\n",
    "\n",
    "    for image, label in unbatched.take(n_samples):\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "        probs = model.predict(image, verbose=0)[0]\n",
    "        pred_index = tf.argmax(probs).numpy()\n",
    "\n",
    "        results.append([\n",
    "            CLASS_NAMES[label.numpy()],\n",
    "            CLASS_NAMES[pred_index],\n",
    "            float(probs[pred_index])\n",
    "        ])\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=[\"true_label\", \"predicted_label\", \"confidence\"]\n",
    "    )\n",
    "\n",
    "    path = os.path.join(OUTPUT_DIR, filename)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Saved predictions to {path}\")\n",
    "\n",
    "\n",
    "predict_and_save(model, val_ds, \"mobilenet_predictions.csv\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Summary\n",
    "# ============================================================\n",
    "#\n",
    "# - We reused a CNN pretrained on ImageNet\n",
    "# - We froze its weights and trained a new classifier\n",
    "# - TFDS handled data loading and labeling\n",
    "# - Results were exported for analysis outside Python\n",
    "#\n",
    "# This same workflow generalizes to larger datasets\n",
    "# and more advanced fine-tuning experiments.\n",
    "#\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5UOm2etrwYCs"
   ],
   "name": "03a_transfer_learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-5.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m75"
  },
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
