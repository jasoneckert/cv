{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.1\n",
      "Dataset loaded:\n",
      "tfds.core.DatasetInfo(\n",
      "    name='tf_flowers',\n",
      "    full_name='tf_flowers/3.0.1',\n",
      "    description=\"\"\"\n",
      "    A large set of images of flowers\n",
      "    \"\"\",\n",
      "    homepage='https://www.tensorflow.org/tutorials/load_data/images',\n",
      "    data_dir='C:\\\\Users\\\\Jason Eckert\\\\tensorflow_datasets\\\\tf_flowers\\\\3.0.1',\n",
      "    file_format=tfrecord,\n",
      "    download_size=218.21 MiB,\n",
      "    dataset_size=221.83 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(None, None, 3), dtype=uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=5),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    nondeterministic_order=False,\n",
      "    splits={\n",
      "        'train': <SplitInfo num_examples=3670, num_shards=2>,\n",
      "    },\n",
      "    citation=\"\"\"@ONLINE {tfflowers,\n",
      "    author = \"The TensorFlow Team\",\n",
      "    title = \"Flowers\",\n",
      "    month = \"jan\",\n",
      "    year = \"2019\",\n",
      "    url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\" }\"\"\",\n",
      ")\n",
      "Training and validation datasets ready.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense_hidden (Dense)        (None, 16)                20496     \n",
      "                                                                 \n",
      " flower_prob (Dense)         (None, 5)                 85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,278,565\n",
      "Trainable params: 1,226,661\n",
      "Non-trainable params: 1,051,904\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "92/92 [==============================] - 35s 341ms/step - loss: 0.7872 - accuracy: 0.7316 - val_loss: 0.4750 - val_accuracy: 0.8488\n",
      "Epoch 2/5\n",
      "92/92 [==============================] - 30s 327ms/step - loss: 0.2337 - accuracy: 0.9319 - val_loss: 0.3771 - val_accuracy: 0.8733\n",
      "Epoch 3/5\n",
      "92/92 [==============================] - 30s 331ms/step - loss: 0.1210 - accuracy: 0.9663 - val_loss: 0.3706 - val_accuracy: 0.8883\n",
      "Epoch 4/5\n",
      "92/92 [==============================] - 32s 344ms/step - loss: 0.0674 - accuracy: 0.9854 - val_loss: 0.2864 - val_accuracy: 0.9183\n",
      "Epoch 5/5\n",
      "92/92 [==============================] - 31s 330ms/step - loss: 0.0340 - accuracy: 0.9952 - val_loss: 0.2900 - val_accuracy: 0.9074\n",
      "Saved training history to 03b_mobilenet_finetune_outputs\\finetune_training_history.csv\n",
      "Saved evaluation artifacts:\n",
      "- 03b_mobilenet_finetune_outputs\\finetune_predictions.csv\n",
      "- 03b_mobilenet_finetune_outputs\\confusion_matrix_raw.csv\n",
      "- 03b_mobilenet_finetune_outputs\\confusion_matrix_normalized.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Fine-Tuning MobileNetV2 (Partial Unfreezing, TFDS, Local)\n",
    "# ============================================================\n",
    "#\n",
    "# This notebook builds on 03a by demonstrating *fine-tuning*:\n",
    "# instead of freezing the entire pretrained CNN, we selectively\n",
    "# unfreeze the top layers of MobileNetV2 and continue training.\n",
    "#\n",
    "# IMPORTANT DESIGN CHOICE:\n",
    "# ------------------------\n",
    "# To keep runtime reasonable on CPU-only student laptops,\n",
    "# we fine-tune ONLY the last N layers of MobileNetV2 rather\n",
    "# than the entire network.\n",
    "#\n",
    "# This approach:\n",
    "# - Preserves most pretrained features\n",
    "# - Reduces computation dramatically\n",
    "# - Clearly illustrates the concept of fine-tuning\n",
    "#\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Imports\n",
    "# ----------------------------\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5               # Fewer epochs for CPU friendliness\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Number of layers (from the top of MobileNetV2) to fine-tune\n",
    "FINE_TUNE_AT = 20        # Option A: fine-tune last N layers\n",
    "\n",
    "CLASS_NAMES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
    "\n",
    "OUTPUT_DIR = \"03b_mobilenet_finetune_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load Dataset (TFDS)\n",
    "# ============================================================\n",
    "\n",
    "(ds_train, ds_val), ds_info = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:80%]\", \"train[80%:]\"],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "print(\"Dataset loaded:\")\n",
    "print(ds_info)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Preprocessing Pipeline\n",
    "# ============================================================\n",
    "#\n",
    "# MobileNetV2 expects:\n",
    "# - 224x224 RGB images\n",
    "# - Float32 inputs\n",
    "# - MobileNetV2-specific preprocessing\n",
    "#\n",
    "# ============================================================\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "train_ds = (\n",
    "    ds_train\n",
    "    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .shuffle(1000)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    ds_val\n",
    "    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(\"Training and validation datasets ready.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Build Fine-Tuned MobileNetV2 Model\n",
    "# ============================================================\n",
    "#\n",
    "# Strategy:\n",
    "# - Load MobileNetV2 pretrained on ImageNet\n",
    "# - Freeze MOST layers\n",
    "# - Unfreeze ONLY the last `FINE_TUNE_AT` layers\n",
    "#\n",
    "# This is called *partial fine-tuning* and is far more\n",
    "# practical than full fine-tuning on CPU hardware.\n",
    "#\n",
    "# ============================================================\n",
    "\n",
    "def build_finetuned_model(\n",
    "    fine_tune_at=20,\n",
    "    learning_rate=1e-4\n",
    "):\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "    )\n",
    "\n",
    "    # Freeze all layers except the top N\n",
    "    for layer in base_model.layers[:-fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    for layer in base_model.layers[-fine_tune_at:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\", name=\"dense_hidden\"),\n",
    "        tf.keras.layers.Dense(len(CLASS_NAMES), activation=\"softmax\", name=\"flower_prob\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_finetuned_model(\n",
    "    fine_tune_at=FINE_TUNE_AT,\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Train Model\n",
    "# ============================================================\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Save Training History (CSV)\n",
    "# ============================================================\n",
    "\n",
    "def save_training_history(history, filename):\n",
    "    df = pd.DataFrame(history.history)\n",
    "    path = os.path.join(OUTPUT_DIR, filename)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Saved training history to {path}\")\n",
    "\n",
    "\n",
    "save_training_history(history, \"finetune_training_history.csv\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Evaluate Predictions and Confusion Matrix\n",
    "# ============================================================\n",
    "#\n",
    "# We export:\n",
    "# - Per-sample predictions\n",
    "# - Raw confusion matrix\n",
    "# - Normalized confusion matrix\n",
    "#\n",
    "# All results are saved as CSV files.\n",
    "#\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_and_save(model, dataset):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for images, labels in dataset:\n",
    "        probs = model.predict(images, verbose=0)\n",
    "        preds = tf.argmax(probs, axis=1)\n",
    "\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(preds.numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Save per-sample predictions\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"true_label\": [CLASS_NAMES[i] for i in y_true],\n",
    "        \"predicted_label\": [CLASS_NAMES[i] for i in y_pred]\n",
    "    })\n",
    "\n",
    "    pred_path = os.path.join(OUTPUT_DIR, \"finetune_predictions.csv\")\n",
    "    pred_df.to_csv(pred_path, index=False)\n",
    "\n",
    "    # Confusion matrix (raw)\n",
    "    cm = tf.math.confusion_matrix(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        num_classes=len(CLASS_NAMES)\n",
    "    ).numpy()\n",
    "\n",
    "    cm_df = pd.DataFrame(cm, index=CLASS_NAMES, columns=CLASS_NAMES)\n",
    "    cm_path = os.path.join(OUTPUT_DIR, \"confusion_matrix_raw.csv\")\n",
    "    cm_df.to_csv(cm_path)\n",
    "\n",
    "    # Confusion matrix (normalized)\n",
    "    cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm_df = pd.DataFrame(cm_norm, index=CLASS_NAMES, columns=CLASS_NAMES)\n",
    "    cm_norm_path = os.path.join(OUTPUT_DIR, \"confusion_matrix_normalized.csv\")\n",
    "    cm_norm_df.to_csv(cm_norm_path)\n",
    "\n",
    "    print(\"Saved evaluation artifacts:\")\n",
    "    print(\"-\", pred_path)\n",
    "    print(\"-\", cm_path)\n",
    "    print(\"-\", cm_norm_path)\n",
    "\n",
    "\n",
    "evaluate_and_save(model, val_ds)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Summary\n",
    "# ============================================================\n",
    "#\n",
    "# - We fine-tuned ONLY the top layers of MobileNetV2\n",
    "# - Most pretrained weights remained frozen\n",
    "# - Training took longer than pure transfer learning\n",
    "# - This approach balances performance and practicality\n",
    "#\n",
    "# In practice, selective fine-tuning is often preferable\n",
    "# to full fine-tuning, especially with limited compute.\n",
    "#\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
