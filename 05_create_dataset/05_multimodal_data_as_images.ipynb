{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "0uUeDqA32K9o",
    "outputId": "27b66765-ee49-4504-f32e-f34776c4f3b4"
   },
   "source": [
    "# Audio → Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio array created!\n",
      "Shape: (16000,), Sample rate: 8000\n",
      "Spectrogram computed via TensorFlow.\n",
      "Spectrogram shape: (124, 129)\n",
      "Spectrogram saved to spectrogram_outputs\\spectrogram1.png\n",
      "Tensor shape ready for ML: (124, 129, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Preloaded audio\n",
    "# -------------------------------\n",
    "sr = 8000\n",
    "duration_seconds = 2\n",
    "num_samples = sr * duration_seconds\n",
    "y = np.random.rand(num_samples).astype(np.float32)\n",
    "print(\"Audio array created!\")\n",
    "print(f\"Shape: {y.shape}, Sample rate: {sr}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Compute Spectrogram via TensorFlow STFT\n",
    "# -------------------------------\n",
    "frame_length = 256\n",
    "frame_step = 128\n",
    "\n",
    "y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "stft = tf.signal.stft(y_tensor, frame_length=frame_length, frame_step=frame_step)\n",
    "spectrogram = tf.abs(stft)\n",
    "spectrogram = tf.math.pow(spectrogram, 0.5)  # approximate log/Mel scale\n",
    "print(\"Spectrogram computed via TensorFlow.\")\n",
    "print(\"Spectrogram shape:\", spectrogram.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Normalize and convert to uint8\n",
    "# -------------------------------\n",
    "spec_min = tf.reduce_min(spectrogram)\n",
    "spec_max = tf.reduce_max(spectrogram)\n",
    "spectrogram_norm = (spectrogram - spec_min) / (spec_max - spec_min)  # 0-1\n",
    "spectrogram_uint8 = tf.image.convert_image_dtype(spectrogram_norm[..., tf.newaxis], tf.uint8)\n",
    "spectrogram_uint8 = tf.squeeze(spectrogram_uint8)  # H x W\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Save as PNG to a subfolder\n",
    "# -------------------------------\n",
    "output_dir = \"spectrogram_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"spectrogram1.png\")\n",
    "\n",
    "# Convert to PIL Image and save\n",
    "img = Image.fromarray(spectrogram_uint8.numpy(), mode='L')\n",
    "img.save(output_path)\n",
    "print(f\"Spectrogram saved to {output_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Optional: Convert to Tensor for ML\n",
    "# -------------------------------\n",
    "spectrogram_tensor = tf.expand_dims(spectrogram, -1)  # H x W x 1\n",
    "print(\"Tensor shape ready for ML:\", spectrogram_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded audio: trumpet.wav\n",
      "Audio shape: (143775,), Sample rate: 44100\n",
      "Spectrogram computed via TensorFlow.\n",
      "Spectrogram shape: (1122, 129)\n",
      "Spectrogram saved to spectrogram_outputs\\trumpet_spectrogram.png\n",
      "Tensor shape ready for ML: (1122, 129, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import soundfile as sf\n",
    "from PIL import Image\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load trumpet.wav\n",
    "# -------------------------------\n",
    "audio_path = \"trumpet.wav\"\n",
    "y, sr = sf.read(audio_path, dtype='float32')  # y.shape = (num_samples,)\n",
    "print(f\"Loaded audio: {audio_path}\")\n",
    "print(f\"Audio shape: {y.shape}, Sample rate: {sr}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Compute Spectrogram via TensorFlow STFT\n",
    "# -------------------------------\n",
    "frame_length = 256\n",
    "frame_step = 128\n",
    "\n",
    "y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "stft = tf.signal.stft(y_tensor, frame_length=frame_length, frame_step=frame_step)\n",
    "spectrogram = tf.abs(stft)\n",
    "spectrogram = tf.math.pow(spectrogram, 0.5)  # approximate log/Mel scale\n",
    "print(\"Spectrogram computed via TensorFlow.\")\n",
    "print(\"Spectrogram shape:\", spectrogram.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Normalize and convert to uint8 for saving\n",
    "# -------------------------------\n",
    "spec_min = tf.reduce_min(spectrogram)\n",
    "spec_max = tf.reduce_max(spectrogram)\n",
    "spectrogram_norm = (spectrogram - spec_min) / (spec_max - spec_min)  # 0-1\n",
    "spectrogram_uint8 = tf.image.convert_image_dtype(spectrogram_norm[..., tf.newaxis], tf.uint8)\n",
    "spectrogram_uint8 = tf.squeeze(spectrogram_uint8)  # H x W\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Save as PNG to a subfolder\n",
    "# -------------------------------\n",
    "output_dir = \"spectrogram_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"trumpet_spectrogram.png\")\n",
    "\n",
    "img = Image.fromarray(spectrogram_uint8.numpy(), mode='L')\n",
    "img.save(output_path)\n",
    "print(f\"Spectrogram saved to {output_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Optional: Convert to Tensor for ML\n",
    "# -------------------------------\n",
    "spectrogram_tensor = tf.expand_dims(spectrogram, -1)  # H x W x 1\n",
    "print(\"Tensor shape ready for ML:\", spectrogram_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text → Embeddings → “Image”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (3, 384)\n",
      "Saved embedding for sentence 1 to sentence_embeddings_outputs\\sentence_1_embedding.npy\n",
      "Saved embedding for sentence 2 to sentence_embeddings_outputs\\sentence_2_embedding.npy\n",
      "Saved embedding for sentence 3 to sentence_embeddings_outputs\\sentence_3_embedding.npy\n",
      "Saved embeddings heatmap to sentence_embeddings_outputs\\embeddings_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Create output folder\n",
    "# -------------------------------\n",
    "output_dir = \"sentence_embeddings_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load SentenceTransformer model\n",
    "# -------------------------------\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Prepare paragraph / sentences\n",
    "# -------------------------------\n",
    "paragraph = \"\"\"\n",
    "Computer vision enables machines to interpret visual information.\n",
    "Deep learning has revolutionized image recognition.\n",
    "Datasets are critical to model performance.\n",
    "\"\"\"\n",
    "\n",
    "sentences = paragraph.strip().split(\"\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Encode sentences\n",
    "# -------------------------------\n",
    "embeddings = model.encode(sentences)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Save each embedding as a .npy file\n",
    "# -------------------------------\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    npy_path = os.path.join(output_dir, f\"sentence_{i+1}_embedding.npy\")\n",
    "    np.save(npy_path, embedding)\n",
    "    print(f\"Saved embedding for sentence {i+1} to {npy_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Save embeddings as a heatmap PNG \n",
    "# -------------------------------\n",
    "# Normalize embeddings to 0-255 for image display\n",
    "emb_min = embeddings.min()\n",
    "emb_max = embeddings.max()\n",
    "emb_norm = ((embeddings - emb_min) / (emb_max - emb_min) * 255).astype(np.uint8)\n",
    "\n",
    "# Convert to PIL image (height = sentences, width = embedding dimension)\n",
    "heatmap_img = Image.fromarray(emb_norm)\n",
    "heatmap_path = os.path.join(output_dir, \"embeddings_heatmap.png\")\n",
    "heatmap_img.save(heatmap_path)\n",
    "print(f\"Saved embeddings heatmap to {heatmap_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion:\n",
    "- Neural networks operate on structured tensors.\n",
    "- Images, audio, and text can all be represented as numeric grids.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "04_audio.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
