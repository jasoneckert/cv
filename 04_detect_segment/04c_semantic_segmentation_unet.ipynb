{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation with U-Net\n",
    "- Understand pixel-wise classification\n",
    "- Build an encoder–decoder network\n",
    "- Understand skip connections\n",
    "- Train a lightweight U-Net\n",
    "\n",
    "## What Is Semantic Segmentation?\n",
    "- Instead of: Image → Label\n",
    "- We can now predict: Image → Mask (same height & width)\n",
    "- Each pixel gets a class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (16, 128, 128, 3)\n",
      "Masks shape: (16, 128, 128, 1)\n",
      "Mask unique values: [1 2 0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# -----------------------------\n",
    "# Load Oxford-IIIT Pet dataset\n",
    "# -----------------------------\n",
    "# Use latest available version (4.0.0)\n",
    "dataset, info = tfds.load(\n",
    "    \"oxford_iiit_pet:4.0.0\",\n",
    "    with_info=True,\n",
    "    as_supervised=False,  # we handle images/masks manually\n",
    "    shuffle_files=True\n",
    ")\n",
    "\n",
    "train_ds = dataset[\"train\"]\n",
    "val_ds = dataset[\"test\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Preprocessing function\n",
    "# -----------------------------\n",
    "def preprocess(sample):\n",
    "    # Resize image & mask\n",
    "    image = tf.image.resize(sample[\"image\"], (128, 128))\n",
    "    mask = tf.image.resize(sample[\"segmentation_mask\"], (128, 128), method=\"nearest\")\n",
    "    \n",
    "    # Normalize image to [0,1]\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    # Ensure mask is integer type and zero-indexed\n",
    "    mask = tf.cast(mask, tf.int32)\n",
    "    mask = mask - 1  # original mask values are 1-3; shift to 0-2\n",
    "    \n",
    "    return image, mask\n",
    "\n",
    "# -----------------------------\n",
    "# Apply preprocessing, batching, prefetching\n",
    "# -----------------------------\n",
    "batch_size = 16\n",
    "\n",
    "train_ds = (\n",
    "    train_ds\n",
    "    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    val_ds\n",
    "    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Quick sanity check\n",
    "# -----------------------------\n",
    "for images, masks in train_ds.take(1):\n",
    "    print(\"Images shape:\", images.shape)\n",
    "    print(\"Masks shape:\", masks.shape)\n",
    "    print(\"Mask unique values:\", tf.unique(tf.reshape(masks, [-1]))[0].numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Lightweight U-Net\n",
    "# We use: MobileNetV2 encoder (pretrained), transposed convolutions for upsampling, skip connections\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(128,128,3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "# Extract skip layers\n",
    "layer_names = [\n",
    "    \"block_1_expand_relu\",\n",
    "    \"block_3_expand_relu\",\n",
    "    \"block_6_expand_relu\",\n",
    "    \"block_13_expand_relu\",\n",
    "    \"block_16_project\"\n",
    "]\n",
    "\n",
    "layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "down_stack.trainable = False\n",
    "\n",
    "# Unsampling path\n",
    "\n",
    "def upsample(filters, size):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding=\"same\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU()\n",
    "    ])\n",
    "\n",
    "# Construct U-Net:\n",
    "\n",
    "inputs = tf.keras.Input(shape=(128,128,3))\n",
    "skips = down_stack(inputs)\n",
    "x = skips[-1]\n",
    "skips = reversed(skips[:-1])\n",
    "\n",
    "up_stack = [\n",
    "    upsample(512, 3),\n",
    "    upsample(256, 3),\n",
    "    upsample(128, 3),\n",
    "    upsample(64, 3)\n",
    "]\n",
    "\n",
    "for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "outputs = tf.keras.layers.Conv2DTranspose(\n",
    "    3, 3, strides=2, padding=\"same\", activation=\"softmax\"\n",
    ")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "200/200 [==============================] - 55s 260ms/step - loss: 0.3674 - accuracy: 0.8593\n",
      "Epoch 2/3\n",
      "200/200 [==============================] - 54s 271ms/step - loss: 0.2778 - accuracy: 0.8927\n",
      "Epoch 3/3\n",
      "200/200 [==============================] - 51s 255ms/step - loss: 0.2607 - accuracy: 0.8984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x216fc0f1a80>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "model.fit(train_ds.take(200), epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 954ms/step\n",
      "Saved sample 0 images to segmentation_results\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Saved sample 1 images to segmentation_results\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Saved sample 2 images to segmentation_results\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Saved sample 3 images to segmentation_results\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Saved sample 4 images to segmentation_results\n"
     ]
    }
   ],
   "source": [
    "# Visualizing Predictions\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# -----------------------------\n",
    "# Directory to save results\n",
    "# -----------------------------\n",
    "RESULTS_DIR = \"segmentation_results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Color mapping for masks\n",
    "# -----------------------------\n",
    "COLOR_MAP = tf.constant([\n",
    "    [0, 0, 0],       # class 0 = background (black)\n",
    "    [255, 0, 0],     # class 1 = class 1 (red)\n",
    "    [0, 255, 0],     # class 2 = class 2 (green)\n",
    "], dtype=tf.uint8)\n",
    "\n",
    "def apply_color_mask(mask_2d):\n",
    "    \"\"\"\n",
    "    Convert 2D mask of class indices to RGB color mask using COLOR_MAP.\n",
    "    \"\"\"\n",
    "    mask_2d = tf.cast(mask_2d, tf.int32)\n",
    "    color_mask = tf.gather(COLOR_MAP, mask_2d)\n",
    "    return tf.cast(color_mask, tf.uint8)\n",
    "\n",
    "# -----------------------------\n",
    "# Function to save a single sample\n",
    "# -----------------------------\n",
    "def save_colored_sample(image, mask, model, index, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Save input image, true mask overlay, and predicted mask overlay.\n",
    "\n",
    "    Args:\n",
    "        image: tf.Tensor, shape (H, W, 3)\n",
    "        mask: tf.Tensor, shape (H, W) or (H, W, 1)\n",
    "        model: trained segmentation model\n",
    "        index: int, sample index for filenames\n",
    "        alpha: float, blending factor for overlay\n",
    "    \"\"\"\n",
    "    # Ensure image is uint8\n",
    "    if image.dtype != tf.uint8:\n",
    "        image_uint8 = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "    else:\n",
    "        image_uint8 = image\n",
    "\n",
    "    # Flatten mask to 2D if needed\n",
    "    mask_2d = mask[...,0] if mask.shape[-1] == 1 else mask\n",
    "\n",
    "    # True mask overlay\n",
    "    true_color_mask = apply_color_mask(mask_2d)\n",
    "    alpha_tf = tf.constant(alpha, dtype=tf.float32)\n",
    "    true_overlay = tf.cast(alpha_tf * tf.cast(true_color_mask, tf.float32) +\n",
    "                           (1 - alpha_tf) * tf.cast(image_uint8, tf.float32),\n",
    "                           tf.uint8)\n",
    "\n",
    "    # Predicted mask overlay\n",
    "    pred_mask_logits = model.predict(tf.expand_dims(image, axis=0))\n",
    "    pred_mask = tf.argmax(pred_mask_logits[0], axis=-1)\n",
    "    pred_color_mask = apply_color_mask(pred_mask)\n",
    "    pred_overlay = tf.cast(alpha_tf * tf.cast(pred_color_mask, tf.float32) +\n",
    "                           (1 - alpha_tf) * tf.cast(image_uint8, tf.float32),\n",
    "                           tf.uint8)\n",
    "\n",
    "    # Save images\n",
    "    tf.io.write_file(os.path.join(RESULTS_DIR, f\"sample_{index}_input.jpg\"),\n",
    "                     tf.io.encode_jpeg(image_uint8))\n",
    "    tf.io.write_file(os.path.join(RESULTS_DIR, f\"sample_{index}_true_overlay.jpg\"),\n",
    "                     tf.io.encode_jpeg(true_overlay))\n",
    "    tf.io.write_file(os.path.join(RESULTS_DIR, f\"sample_{index}_pred_overlay.jpg\"),\n",
    "                     tf.io.encode_jpeg(pred_overlay))\n",
    "\n",
    "    print(f\"Saved sample {index} images to {RESULTS_DIR}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Loop to save N samples from dataset\n",
    "# -----------------------------\n",
    "N = 5  # number of samples to save\n",
    "sample_count = 0\n",
    "\n",
    "for images_batch, masks_batch in train_ds:\n",
    "    batch_size = images_batch.shape[0]\n",
    "    for i in range(batch_size):\n",
    "        if sample_count >= N:\n",
    "            break\n",
    "        save_colored_sample(images_batch[i], masks_batch[i], model, index=sample_count)\n",
    "        sample_count += 1\n",
    "    if sample_count >= N:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Final | Conceptual | Comparison |\n",
    "|---|---|---|\n",
    "| Classification | One label | Low |\n",
    "| Detection | Boxes + labels | Medium |\n",
    "| Segmentation | Pixel-level mask\t| High |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
