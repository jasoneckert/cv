{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8HQYsAtC0Fv"
   },
   "source": [
    "# Distributed Training Concepts with tf.distribute\n",
    "- As datasets and models grow, single-device training becomes too slow.\n",
    "- Production ML systems often train on multiple GPUs and systems (or clusters of systems)\n",
    "- TensorFlow provides a unified API to support this: tf.distribute\n",
    "\n",
    "Even if you only have a CPU today, you can still write code that scales tomorrow - we'll:\n",
    "- Introduce data parallelism\n",
    "- Use MirroredStrategy\n",
    "- Train inside a distribution scope\n",
    "- Compare batch size scaling\n",
    "- Understand how distribution changes system behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugGJcxKAwhc2",
    "outputId": "8e946159-46cf-4aba-f53e-622e9ea8adee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.1\n",
      "Available devices:\n",
      " - PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "print(\"Available devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(\" -\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In data parallel training:\n",
    "- Each device gets a copy of the model\n",
    "- Each device processes a different batch slice\n",
    "- Gradients are averaged\n",
    "- Weights are synchronized\n",
    "\n",
    "Conceptually: Batch → Split → Parallel Compute → Aggregate Gradients → Update\n",
    "\n",
    "TensorFlow handles this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of replicas in sync: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize Strategy (On 1 device → 1 replica, on 2 GPUs → 2 replicas)\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(\"Number of replicas in sync:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "# Load Dataset\n",
    "\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "(ds_train, ds_val), ds_info = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:80%]\", \"train[80%:]\"],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "NUM_CLASSES = ds_info.features[\"label\"].num_classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "ds_train = (\n",
    "    ds_train\n",
    "    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "ds_val = (\n",
    "    ds_val\n",
    "    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 2.5640 - accuracy: 0.4196\n",
      "Epoch 2/2\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 1.0007 - accuracy: 0.6008\n",
      "Baseline training time: 16.231674671173096\n"
     ]
    }
   ],
   "source": [
    "# Model WITHOUT Distribution (Baseline)\n",
    "\n",
    "# Build Baseline Model\n",
    "\n",
    "baseline_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation=\"relu\", input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "baseline_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train Baseline\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "baseline_model.fit(ds_train, epochs=2)\n",
    "\n",
    "baseline_time = time.time() - start\n",
    "print(\"Baseline training time:\", baseline_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "92/92 [==============================] - 13s 115ms/step - loss: 2.9970 - accuracy: 0.4043\n",
      "Epoch 2/2\n",
      "92/92 [==============================] - 10s 108ms/step - loss: 1.0398 - accuracy: 0.5722\n",
      "Distributed training time: 23.187735557556152\n"
     ]
    }
   ],
   "source": [
    "# Model WITH Distribution Strategy\n",
    "# To do this, we build model inside strategy.scope()\n",
    "\n",
    "# Build Distributed Model\n",
    "\n",
    "with strategy.scope():\n",
    "    dist_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation=\"relu\", input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    dist_model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "# Train Distributed Model\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "dist_model.fit(ds_train, epochs=2)\n",
    "\n",
    "dist_time = time.time() - start\n",
    "print(\"Distributed training time:\", dist_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size Scaling Concept\n",
    "\n",
    "When using multiple replicas, effective global batch size = per_replica_batch_size × num_replicas\n",
    "\n",
    "So if: Batch = 32 and Replicas = 2, then Global batch = 64\n",
    "\n",
    "This often requires:\n",
    "- Learning rate adjustment\n",
    "- Monitoring convergence behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replicas: 1\n",
      "Per-replica batch size: 32\n",
      "Effective global batch size: 32\n"
     ]
    }
   ],
   "source": [
    "print(\"Replicas:\", strategy.num_replicas_in_sync)\n",
    "print(\"Per-replica batch size:\", BATCH_SIZE)\n",
    "print(\"Effective global batch size:\", BATCH_SIZE * strategy.num_replicas_in_sync)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5UOm2etrwYCs"
   ],
   "name": "03a_transfer_learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
