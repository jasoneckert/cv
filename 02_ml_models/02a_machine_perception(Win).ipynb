{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "a8HQYsAtC0Fv"
   },
   "source": [
    "## Flowers Image Classification using a Linear Model\n",
    "\n",
    "In this notebook, we will use a creative-commons licensed flower photo dataset\n",
    "containing **3,670 images** across **5 categories**:\n",
    "\n",
    "- daisy\n",
    "- dandelion\n",
    "- roses\n",
    "- sunflowers\n",
    "- tulips\n",
    "\n",
    "Rather than relying on cloud storage, we will load this dataset locally using\n",
    "`tensorflow_datasets`, which provides a standardized and reproducible way to\n",
    "access common machine learning datasets.\n",
    "\n",
    "The dataset will be:\n",
    "- Downloaded once (if not already cached)\n",
    "- Stored locally in the `data/tfds/` directory\n",
    "- Split into training (80%) and validation (20%) subsets\n",
    "\n",
    "This setup allows the notebook to run fully offline after the initial download\n",
    "and ensures consistent results across student machines.\n",
    "\n",
    "### üìÅ Visual Outputs\n",
    "\n",
    "To ensure this notebook runs reliably on Windows, all figures are **saved to disk**\n",
    "instead of being displayed inline.\n",
    "\n",
    "‚û°Ô∏è After running the notebook, open the `outputs/` folder to view:\n",
    "- example images\n",
    "- training curves\n",
    "- predictions\n",
    "- learned weights\n",
    "\n",
    "If you are on macOS/Linux, you may replace savefig() with plt.show() to see the images without saving them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cGXtfo5CcXi",
    "outputId": "dcab7c7f-a6c8-4ad3-c692-1f0a6a15e34c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: C:\\Users\\Jason Eckert\\Documents\\cv\\02_ml_models\\data\n",
      "Output directory:  C:\\Users\\Jason Eckert\\Documents\\cv\\02_ml_models\\outputs\n",
      "Number of classes: 5\n",
      "Class names: ['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']\n",
      "Training samples: 3670\n",
      "Validation samples: 734\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Step 0: Imports and environment setup\n",
    "# ========================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import csv\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# -------------------------------\n",
    "# Windows TensorFlow stability\n",
    "# -------------------------------\n",
    "if platform.system() == \"Windows\":\n",
    "    # Disable GPU to prevent native crashes\n",
    "    try:\n",
    "        tf.config.set_visible_devices([], 'GPU')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Limit threading to avoid MKL / OpenMP conflicts\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "# -------------------------------\n",
    "# Output directories\n",
    "# -------------------------------\n",
    "DATA_DIR = Path(\"data\")\n",
    "OUT_DIR = Path(\"outputs\")\n",
    "\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Dataset directory: {DATA_DIR.resolve()}\")\n",
    "print(f\"Output directory:  {OUT_DIR.resolve()}\")\n",
    "\n",
    "# ========================================================\n",
    "# Step 1: Load the TF Flowers dataset\n",
    "# ========================================================\n",
    "\n",
    "(ds_train, ds_val), ds_info = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:80%]\", \"train[80%:]\"],\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    "    data_dir=DATA_DIR / \"tfds\"\n",
    ")\n",
    "\n",
    "CLASS_NAMES = ds_info.features[\"label\"].names\n",
    "NUM_CLASSES = ds_info.features[\"label\"].num_classes\n",
    "\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(\"Class names:\", CLASS_NAMES)\n",
    "print(f\"Training samples: {ds_info.splits['train'].num_examples}\")\n",
    "print(f\"Validation samples: {int(ds_info.splits['train'].num_examples * 0.2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "vniHwq0CEA-x",
    "outputId": "baf52f6b-e202-4a4f-c09f-8837082c0fe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving one example image per class ---\n",
      "Saved example for class 'dandelion'\n",
      "Saved example for class 'daisy'\n",
      "Saved example for class 'tulips'\n",
      "Saved example for class 'sunflowers'\n",
      "Saved example for class 'roses'\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Step 2: Display a few images safely (Windows)\n",
    "# ========================================================\n",
    "print(\"\\n--- Saving one example image per class ---\")\n",
    "\n",
    "for idx, class_name in enumerate(CLASS_NAMES):\n",
    "    class_ds = ds_train.filter(lambda img, lbl: lbl == idx)\n",
    "    for image, label in class_ds.take(1):\n",
    "        img_uint8 = image.numpy().astype(\"uint8\")\n",
    "        img_pil = Image.fromarray(img_uint8)\n",
    "        img_pil.save(OUT_DIR / f\"class_example_{class_name}.png\")\n",
    "        print(f\"Saved example for class '{class_name}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Bx3sWegFj0s"
   },
   "source": [
    "## A simple rule-based model\n",
    "\n",
    "Let's get the average color of RGB values in the different\n",
    "types of flowers and then classify an unknown image as\n",
    "belonging to closest centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FscN0NcDGCV9",
    "outputId": "6af8dbe8-dea7-4c02-f5c9-3a24c94e4b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Per-image average RGB (first 3 images) ---\n",
      "tulips [156.44687   72.975204  31.295033]\n",
      "sunflowers [82.408905 57.083298 19.67777 ]\n",
      "sunflowers [125.994156 142.72287   94.961655]\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Step 3: Compute average RGB per image (rule-based centroid)\n",
    "# ========================================================\n",
    "print(\"\\n--- Per-image average RGB (first 3 images) ---\")\n",
    "\n",
    "for image, label in ds_train.take(3):\n",
    "    avg_color = tf.reduce_mean(tf.cast(image, tf.float32), axis=[0, 1])\n",
    "    print(CLASS_NAMES[int(label)], avg_color.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Step 4a: Define Centroid and CentroidRule classes\n",
    "# ========================================================\n",
    "\n",
    "class Centroid:\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n",
    "        self.sum_so_far = tf.constant(0., dtype=tf.float32)\n",
    "        self.count_so_far = 0\n",
    "\n",
    "    def update(self, value):\n",
    "        self.sum_so_far += value\n",
    "        self.count_so_far += 1\n",
    "        if self.count_so_far % 100 == 0:\n",
    "            print(self.label, self.count_so_far)\n",
    "\n",
    "    def centroid(self):\n",
    "        return self.sum_so_far / self.count_so_far\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.label} {self.centroid().numpy()}'\n",
    "\n",
    "\n",
    "class CentroidRule:\n",
    "    def __init__(self):\n",
    "        self.centroids = {f: Centroid(f) for f in CLASS_NAMES}\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        for img, label in dataset:\n",
    "            label_name = CLASS_NAMES[int(label)]\n",
    "            avg = tf.reduce_mean(tf.cast(img, tf.float32), axis=[0, 1])\n",
    "            self.centroids[label_name].update(avg)\n",
    "\n",
    "    def predict(self, img):\n",
    "        avg = tf.reduce_mean(tf.cast(img, tf.float32), axis=[0, 1])\n",
    "        best_label = \"\"\n",
    "        best_diff = float(\"inf\")\n",
    "\n",
    "        for key, val in self.centroids.items():\n",
    "            diff = tf.reduce_sum(tf.abs(avg - val.centroid()))\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_label = key\n",
    "\n",
    "        return best_label\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        correct, total = 0, 0\n",
    "        for img, label in dataset:\n",
    "            if self.predict(img) == CLASS_NAMES[int(label)]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VLo7XIhBMELn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dandelion 100\n",
      "tulips 100\n",
      "\n",
      "Centroid for daisy: daisy [103.03394  110.325645  86.47161 ]\n",
      "Centroid for roses: roses [127.029045  96.59135   87.69432 ]\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Step 4b: Fit centroid classifier on small subsets (fast demo)\n",
    "# ========================================================\n",
    "train_subset = ds_train.take(500)\n",
    "eval_subset = ds_val.take(50)\n",
    "\n",
    "rule = CentroidRule()\n",
    "rule.fit(train_subset)\n",
    "\n",
    "print(\"\\nCentroid for daisy:\", rule.centroids[\"daisy\"])\n",
    "print(\"Centroid for roses:\", rule.centroids[\"roses\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eC3QOxOqNSNK",
    "outputId": "6b50aff0-9c00-44ae-e472-e57f2e5a9d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Centroid-rule accuracy: 0.36\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Step 4c: Evaluate classifier on small validation subset\n",
    "# ========================================================\n",
    "\n",
    "accuracy = rule.evaluate(eval_subset)\n",
    "print(\"\\nCentroid-rule accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Rule-Based Predictions (Validation set) ---\n",
      "Example 1\n",
      "  True label:      roses\n",
      "  Predicted label: dandelion\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Step 5a: Using the model to predict one image\n",
    "# ========================================================\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Directory to save example images\n",
    "RULE_OUTPUT_DIR = OUT_DIR / \"rule_based_examples\"\n",
    "RULE_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\n--- Rule-Based Predictions (Validation set) ---\")\n",
    "\n",
    "# Take one example\n",
    "for idx, (img, label) in enumerate(ds_val.take(1)):\n",
    "    predicted_label = rule.predict(img)\n",
    "    true_label = CLASS_NAMES[int(label)]\n",
    "    print(f\"Example {idx+1}\")\n",
    "    print(\"  True label:     \", true_label)\n",
    "    print(\"  Predicted label:\", predicted_label)\n",
    "\n",
    "    # Save the image with predicted label in filename\n",
    "    img_pil = Image.fromarray(img.numpy().astype(\"uint8\"))\n",
    "    img_pil.save(RULE_OUTPUT_DIR / f\"example_{idx+1}_{true_label}_pred_{predicted_label}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch example 1\n",
      "  True label:      roses\n",
      "  Predicted label: dandelion\n",
      "Batch example 2\n",
      "  True label:      tulips\n",
      "  Predicted label: dandelion\n",
      "Batch example 3\n",
      "  True label:      tulips\n",
      "  Predicted label: dandelion\n",
      "Batch example 4\n",
      "  True label:      tulips\n",
      "  Predicted label: roses\n",
      "Batch example 5\n",
      "  True label:      daisy\n",
      "  Predicted label: dandelion\n",
      "\n",
      "Saved rule-based example images to C:\\Users\\Jason Eckert\\Documents\\cv\\02_ml_models\\outputs\\rule_based_examples\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Step 5b: Using the model to predict a small batch\n",
    "# ========================================================\n",
    "\n",
    "# Take a small batch of 5 examples\n",
    "for idx, (img, label) in enumerate(ds_val.take(5)):\n",
    "    predicted_label = rule.predict(img)\n",
    "    true_label = CLASS_NAMES[int(label)]\n",
    "    print(f\"Batch example {idx+1}\")\n",
    "    print(\"  True label:     \", true_label)\n",
    "    print(\"  Predicted label:\", predicted_label)\n",
    "\n",
    "    # Save image\n",
    "    img_pil = Image.fromarray(img.numpy().astype(\"uint8\"))\n",
    "    img_pil.save(RULE_OUTPUT_DIR / f\"batch_{idx+1}_{true_label}_pred_{predicted_label}.png\")\n",
    "\n",
    "print(f\"\\nSaved rule-based example images to {RULE_OUTPUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75_Hf3JnG1kf"
   },
   "source": [
    "## A linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Step 6: Prepare datasets for linear model\n",
    "# ========================================================\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 224, 224, 3\n",
    "\n",
    "def preprocess(img, label):\n",
    "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    return img, label\n",
    "\n",
    "train_dataset = ds_train.map(preprocess).batch(BATCH_SIZE)\n",
    "eval_dataset = ds_val.map(preprocess).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 150528)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 752645    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 752,645\n",
      "Trainable params: 752,645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Step 7: Define linear model\n",
    "# ========================================================\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "294/294 [==============================] - 4s 11ms/step - loss: 4893.7021 - accuracy: 0.3331 - val_loss: 2577.3735 - val_accuracy: 0.4046\n",
      "Epoch 2/10\n",
      "294/294 [==============================] - 2s 6ms/step - loss: 3675.8147 - accuracy: 0.4074 - val_loss: 6270.2393 - val_accuracy: 0.3815\n",
      "Epoch 3/10\n",
      "294/294 [==============================] - 2s 6ms/step - loss: 3215.9446 - accuracy: 0.4493 - val_loss: 5716.2520 - val_accuracy: 0.4155\n",
      "Epoch 4/10\n",
      "294/294 [==============================] - 2s 7ms/step - loss: 3752.8423 - accuracy: 0.4636 - val_loss: 6338.2432 - val_accuracy: 0.4237\n",
      "Epoch 5/10\n",
      "294/294 [==============================] - 2s 6ms/step - loss: 3619.4319 - accuracy: 0.5000 - val_loss: 5228.4443 - val_accuracy: 0.3733\n",
      "Epoch 6/10\n",
      "294/294 [==============================] - 2s 6ms/step - loss: 3054.1499 - accuracy: 0.5303 - val_loss: 5253.6763 - val_accuracy: 0.3992\n",
      "Epoch 7/10\n",
      "294/294 [==============================] - 2s 6ms/step - loss: 3614.0764 - accuracy: 0.5204 - val_loss: 8435.5479 - val_accuracy: 0.4019\n",
      "Epoch 8/10\n",
      "294/294 [==============================] - 2s 7ms/step - loss: 3627.9956 - accuracy: 0.5416 - val_loss: 4848.8643 - val_accuracy: 0.4387\n",
      "Epoch 9/10\n",
      "294/294 [==============================] - 2s 7ms/step - loss: 4530.0054 - accuracy: 0.5255 - val_loss: 4703.9541 - val_accuracy: 0.4360\n",
      "Epoch 10/10\n",
      "294/294 [==============================] - 2s 7ms/step - loss: 2760.6421 - accuracy: 0.5899 - val_loss: 7227.0522 - val_accuracy: 0.4087\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Step 8: Train the linear model\n",
    "# ========================================================\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=eval_dataset,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Excel-friendly training metrics to outputs\\training_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Step 9: Save training curves as CSV (open/plot in Excel after)\n",
    "# ========================================================\n",
    "import csv\n",
    "\n",
    "# CSV path\n",
    "csv_path = OUT_DIR / \"training_metrics.csv\"\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = len(history.history[\"loss\"])\n",
    "\n",
    "# Create CSV in \"Excel-friendly\" layout\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    # Header row: one column per metric\n",
    "    writer.writerow([\"Epoch\", \"Loss\", \"Validation Loss\", \"Accuracy\", \"Validation Accuracy\"])\n",
    "\n",
    "    # Write row per epoch\n",
    "    for i in range(num_epochs):\n",
    "        writer.writerow([\n",
    "            i + 1,  # Epoch number (1-based)\n",
    "            history.history[\"loss\"][i],\n",
    "            history.history[\"val_loss\"][i],\n",
    "            history.history[\"accuracy\"][i],\n",
    "            history.history[\"val_accuracy\"][i],\n",
    "        ])\n",
    "\n",
    "print(f\"Saved Excel-friendly training metrics to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Excel:\n",
    "1. Open outputs/training_metrics.csv.\n",
    "2. Select all rows including the header.\n",
    "3. Insert ‚Üí Charts ‚Üí Line chart.\n",
    "\n",
    "For Loss plot:\n",
    "- y-axis = Loss / Validation Loss\n",
    "- x-axis = Epoch\n",
    "\n",
    "For Accuracy plot:\n",
    "- y-axis = Accuracy / Validation Accuracy\n",
    "- x-axis = Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prediction grid with labels and confidence to outputs/linear_model_predictions.png\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Step 10: Save predictions as image grid (PIL)\n",
    "# ========================================================\n",
    "\n",
    "from PIL import ImageDraw, ImageFont\n",
    "\n",
    "def save_predictions_pil(dataset, num_images=15):\n",
    "    # Take first num_images from dataset\n",
    "    images, labels = next(iter(dataset.unbatch().batch(num_images)))\n",
    "    preds = model.predict(images, verbose=0)\n",
    "\n",
    "    grid_w, grid_h = 5, 3  # 5 columns, 3 rows\n",
    "    img_h, img_w = images.shape[1], images.shape[2]\n",
    "\n",
    "    canvas = Image.new(\"RGB\", (grid_w * img_w, grid_h * img_h), \"white\")\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "\n",
    "    # Try to load a default font, fallback if not available\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 18)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    for idx in range(num_images):\n",
    "        img = Image.fromarray(images[idx].numpy().astype(\"uint8\"))\n",
    "        row, col = idx // grid_w, idx % grid_w\n",
    "        x0, y0 = col * img_w, row * img_h\n",
    "\n",
    "        # Paste the image into the canvas\n",
    "        canvas.paste(img, (x0, y0))\n",
    "\n",
    "        # Prediction info\n",
    "        pred_probs = preds[idx]\n",
    "        pred_idx = np.argmax(pred_probs)\n",
    "        pred_label = CLASS_NAMES[pred_idx]\n",
    "        true_label = CLASS_NAMES[int(labels[idx])]\n",
    "        confidence = pred_probs[pred_idx]\n",
    "\n",
    "        text = f\"{true_label} ‚Üí {pred_label} ({confidence:.2f})\"\n",
    "\n",
    "        # Measure text size (future-proof)\n",
    "        try:\n",
    "            bbox = draw.textbbox((0, 0), text, font=font)\n",
    "            text_w = bbox[2] - bbox[0]\n",
    "            text_h = bbox[3] - bbox[1]\n",
    "        except AttributeError:\n",
    "            text_w, text_h = draw.textsize(text, font=font)\n",
    "\n",
    "        padding = 4\n",
    "        box_coords = [\n",
    "            x0,\n",
    "            y0,\n",
    "            x0 + text_w + 2 * padding,\n",
    "            y0 + text_h + 2 * padding,\n",
    "        ]\n",
    "\n",
    "        # Draw background rectangle and text\n",
    "        draw.rectangle(box_coords, fill=(0, 0, 0))\n",
    "        draw.text(\n",
    "            (x0 + padding, y0 + padding),\n",
    "            text,\n",
    "            fill=(255, 255, 255),\n",
    "            font=font\n",
    "        )\n",
    "\n",
    "    # Save final canvas\n",
    "    canvas.save(OUT_DIR / \"linear_model_predictions.png\")\n",
    "    print(\"Saved prediction grid with labels and confidence to outputs/linear_model_predictions.png\")\n",
    "\n",
    "# Run it\n",
    "save_predictions_pil(eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained weight visualizations.\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Step 11: Visualize trained linear weights (PIL)\n",
    "# ========================================================\n",
    "def save_trained_weights_pil(model):\n",
    "    WEIGHT_TYPE = 0\n",
    "    LAYER = 1\n",
    "\n",
    "    for flower in range(NUM_CLASSES):\n",
    "        weights = model.layers[LAYER].get_weights()[WEIGHT_TYPE][:, flower]\n",
    "        weights = weights.reshape(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "\n",
    "        # Downsample for clarity and stability\n",
    "        weights = tf.image.resize(weights, [56, 56]).numpy()\n",
    "\n",
    "        min_wt, max_wt = weights.min(), weights.max()\n",
    "        weights = (weights - min_wt) / (max_wt - min_wt + 1e-8)\n",
    "\n",
    "        img = Image.fromarray((weights * 255).astype(\"uint8\"))\n",
    "        img.save(OUT_DIR / f\"weights_{CLASS_NAMES[flower]}.png\")\n",
    "\n",
    "save_trained_weights_pil(model)\n",
    "print(\"Saved trained weight visualizations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Diagram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved softmax diagram to C:\\Users\\Jason Eckert\\Documents\\cv\\02_ml_models\\outputs\\softmax_diagram.png\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example data\n",
    "inx = [\n",
    "    [0, 0.09, 0.06, 0.85, 0],\n",
    "    [0.1, 0.1, 0.7, 0.1, 0.1],\n",
    "    [0, 0.2, 0.4, 0.2, 0],\n",
    "    [0.1, 0.1, 0.4, 0.5, 0.1],\n",
    "    [0.2, 0.2, 0.8, 0.2, 0.2],\n",
    "]\n",
    "\n",
    "labels = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "# Safe logit function\n",
    "def logit(x):\n",
    "    # Clip x to avoid divide-by-zero or log(0)\n",
    "    x = tf.clip_by_value(x, 1e-7, 1 - 1e-7)\n",
    "    return -tf.math.log(1. / x - 1.)\n",
    "\n",
    "bar_width = 30\n",
    "gap = 20\n",
    "num_examples = len(inx)\n",
    "num_categories = len(labels)\n",
    "height_scale = 300  # pixels per 1.0 probability\n",
    "\n",
    "# Canvas size\n",
    "canvas_w = num_categories * 2 * bar_width + (num_categories + 1) * gap\n",
    "canvas_h = num_examples * (height_scale + 50)\n",
    "canvas = Image.new(\"RGB\", (canvas_w, canvas_h), \"white\")\n",
    "draw = ImageDraw.Draw(canvas)\n",
    "\n",
    "# Font\n",
    "try:\n",
    "    font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "except:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "for i, x in enumerate(inx):\n",
    "    prob = np.array(x) / np.sum(x)\n",
    "    logits = logit(prob)\n",
    "    softmax = tf.nn.softmax(logits).numpy()\n",
    "\n",
    "    for j, (p, s) in enumerate(zip(prob, softmax)):\n",
    "        x0 = gap + j * (2 * bar_width + gap)\n",
    "        y0_prob = i * (height_scale + 50) + height_scale - int(p * height_scale)\n",
    "        y0_soft = i * (height_scale + 50) + height_scale - int(s * height_scale)\n",
    "        y1 = i * (height_scale + 50) + height_scale\n",
    "\n",
    "        # Draw original prob (blue)\n",
    "        draw.rectangle([x0, y0_prob, x0 + bar_width, y1], fill=(50, 100, 200))\n",
    "        # Draw softmax (red)\n",
    "        draw.rectangle([x0 + bar_width, y0_soft, x0 + 2*bar_width, y1], fill=(200, 50, 50))\n",
    "\n",
    "        # Optionally add numeric values above bars\n",
    "        draw.text((x0, y0_prob - 18), f\"{p:.2f}\", fill=\"black\", font=font)\n",
    "        draw.text((x0 + bar_width, y0_soft - 18), f\"{s:.2f}\", fill=\"black\", font=font)\n",
    "\n",
    "        # Label categories on first row\n",
    "        if i == 0:\n",
    "            draw.text((x0 + 5, y1 + 2), labels[j], fill=\"black\", font=font)\n",
    "\n",
    "# Save the final diagram\n",
    "out_path = OUT_DIR / \"softmax_diagram.png\"\n",
    "canvas.save(out_path)\n",
    "print(f\"Saved softmax diagram to {out_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "02a_machine_perception.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
